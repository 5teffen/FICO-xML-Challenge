{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "import numpy.polynomial.polynomial as poly\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "import copy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"xML Challenge Dataset and Data Dictionary/heloc_dataset_v1.csv\")\n",
    "feature_names = list(df)\n",
    "data = df.values\n",
    "\n",
    "# --- Converting target to binary --- \n",
    "np.place(data, data == \"Bad\", 0)\n",
    "np.place(data, data == \"Good\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Shift categorical features so they match  ---\n",
    "\n",
    "# Assumes array without labels where MaxDelq2PublicRecLast12M is in [10] and MaxDelqEver is in [11]\n",
    "\n",
    "# def shift_categorical(data):\n",
    "#     col_ten = data[:,10]\n",
    "#     np.place(col_ten, col_ten == 5, 6)\n",
    "#     np.place(col_ten, col_ten == 7, 5)\n",
    "#     np.place(col_ten, col_ten == 8, 7)\n",
    "#     np.place(col_ten, col_ten == 9, 7)\n",
    "    \n",
    "#     col_eleven = data[:,11]\n",
    "#     np.place(col_eleven, col_eleven == 2, 0)\n",
    "#     np.place(col_eleven, col_eleven == 4, 2)\n",
    "#     np.place(col_eleven, col_eleven == 6, 4)\n",
    "#     np.place(col_eleven, col_eleven == 7, 6)\n",
    "#     np.place(col_eleven, col_eleven == 9, 7)\n",
    "#     np.place(col_eleven, col_eleven == 1, 7)\n",
    "#     np.place(col_eleven, col_eleven == 3, 1)\n",
    "#     np.place(col_eleven, col_eleven == 5, 3)\n",
    "#     np.place(col_eleven, col_eleven == 8, 5)\n",
    "\n",
    "#     data[:,10] = col_ten\n",
    "#     data[:,11] = col_eleven\n",
    "    \n",
    "#     return data\n",
    "\n",
    "\n",
    "\n",
    "def shift_categorical(data):\n",
    "    col_ten = data[:,10]\n",
    "    np.place(col_ten, col_ten == 1, 100) # hold\n",
    "    np.place(col_ten, col_ten == 6, 1)\n",
    "    np.place(col_ten, col_ten == 5, 1)\n",
    "    np.place(col_ten, col_ten == 4, 6)\n",
    "    np.place(col_ten, col_ten == 3, 5)\n",
    "    np.place(col_ten, col_ten == 2, 4)\n",
    "    np.place(col_ten, col_ten == 100, 3)\n",
    "    np.place(col_ten, col_ten == 0, 2)\n",
    "    np.place(col_ten, col_ten == 8, 0)\n",
    "    np.place(col_ten, col_ten == 9, 0)\n",
    "    \n",
    "    col_eleven = data[:,11]\n",
    "    np.place(col_eleven, col_eleven == 1, 0)\n",
    "    np.place(col_eleven, col_eleven == 9, 0)\n",
    "    np.place(col_eleven, col_eleven == 7, 1)\n",
    "    np.place(col_eleven, col_eleven == 8, 7)\n",
    "\n",
    "    data[:,10] = col_ten\n",
    "    data[:,11] = col_eleven\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Basic Functions for Matrix editing --- \n",
    "\n",
    "def remove_row_with_all_same_val(data, target, val):\n",
    "    row_no = 0 \n",
    "    for row in data:\n",
    "        for col in row:\n",
    "            if (col == val):\n",
    "                remove = True\n",
    "            else:\n",
    "                remove = False\n",
    "                break\n",
    "\n",
    "        if remove:\n",
    "            data = np.delete(data, row_no, 0)\n",
    "            target = np.delete(target, row_no, 0)     \n",
    "\n",
    "        else:\n",
    "            row_no += 1\n",
    "    return data,target\n",
    "\n",
    "def remove_row_with_vals(data, target, vals):\n",
    "    row_no = 0 \n",
    "    for row in data:\n",
    "        for col in row:\n",
    "            if (col in vals):\n",
    "                data = np.delete(data, row_no, 0)\n",
    "                target = np.delete(target, row_no, 0) \n",
    "                row_no -= 1\n",
    "                break\n",
    "        row_no += 1\n",
    "    return data,target\n",
    "\n",
    "\n",
    "def remove_col_with_vals(data, vals):\n",
    "    no_cols = data.shape[1]\n",
    "    no_rows = data.shape[0]\n",
    "    row = 0\n",
    "    while (no_rows > row):\n",
    "        col = 0\n",
    "        while (no_cols > col):\n",
    "            if (data[row][col] in vals):\n",
    "                data = np.delete(data, col, 1)\n",
    "                no_cols -= 1\n",
    "            else:\n",
    "                col += 1\n",
    "        row += 1     \n",
    "    return data\n",
    "\n",
    "\n",
    "def scaled_row(row):\n",
    "    scld = []\n",
    "    for k in range(features):\n",
    "        scld.append((row[k] - scaler.mean_[k])/scaler.scale_[k])\n",
    "    scld = np.array(scld)\n",
    "    return scld\n",
    "        \n",
    "        \n",
    "def masked_arr(A, mask):\n",
    "    B = []\n",
    "    for i in range(len(A)):\n",
    "        row = []\n",
    "        for j in range(len(A[0])):\n",
    "            if mask[j] != 0:\n",
    "                row.append(A[i][j])\n",
    "        B.append(row)\n",
    "    B = np.array(B)\n",
    "    return B\n",
    "\n",
    "\n",
    "def distance(row1, row2):\n",
    "    dist = 0\n",
    "    for i in range(len(row1)):\n",
    "        t = (row1[i]-row2[i])**2\n",
    "        dist += t\n",
    "    dist = np.sqrt(dist)\n",
    "    return dist\n",
    "\n",
    "# predict features using kNN imputation\n",
    "# need to test for both weighted and simple mean\n",
    "# using 3 or 5 neighbors\n",
    "def predict_feature_weighted(row, C, k, originalArr, ft_idx):\n",
    "    \n",
    "    distances = []\n",
    "    for i in range(len(C)):\n",
    "        distances.append(distance(row,C[i]))\n",
    "    distances = np.array(distances)\n",
    "    \n",
    "    max_dist = np.max(distances)\n",
    "        \n",
    "    idx = np.argpartition(distances, k)\n",
    "\n",
    "    values = []\n",
    "    min_dists = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        values.append(originalArr[idx[i]][ft_idx])\n",
    "#         print(Z[idx[i]])\n",
    "        min_dists.append(distances[idx[i]])\n",
    "    values = np.array(values) \n",
    "    min_dists = np.array(min_dists)\n",
    "\n",
    "    weights = []\n",
    "    for i in min_dists:\n",
    "        weights.append(1 - (i/max_dist))\n",
    "        \n",
    "    imputed_val = 0\n",
    "    for i in range(len(weights)):\n",
    "        imputed_val += weights[i] * values[i]\n",
    "        \n",
    "    return imputed_val    \n",
    "\n",
    "\n",
    "def predict_feature_mean(row, C, k, originalArr, ft_idx):\n",
    "    \n",
    "    distances = []\n",
    "    for i in range(len(C)):\n",
    "        distances.append(distance(row,C[i]))\n",
    "    distances = np.array(distances)\n",
    "        \n",
    "    idx = np.argpartition(distances, k)\n",
    "    \n",
    "    values = []\n",
    "    min_dists = []\n",
    "    for i in range(k):\n",
    "        values.append(originalArr[idx[i]][ft_idx])\n",
    "#         print(Z[idx[i]])\n",
    "        min_dists.append(distances[idx[i]])\n",
    "    values = np.array(values) \n",
    "    min_dists = np.array(min_dists)\n",
    "        \n",
    "    imputed_val = 0\n",
    "    for i in range(len(values)):\n",
    "        imputed_val += values[i]/(len(values))\n",
    "        \n",
    "    return imputed_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = shift_categorical(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Removing the rows with only -9 values ---\n",
    "y_original= data[:,:1]\n",
    "X_original = data[:,1:]\n",
    "X, y = remove_row_with_all_same_val(X_original,y_original,-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Taking the array without any negatives present ---\n",
    "\n",
    "samples, features = X.shape\n",
    "\n",
    "X_good = []\n",
    "\n",
    "for i in range(samples):\n",
    "    remove = False\n",
    "    for j in range(features):\n",
    "        if X[i][j] < 0:\n",
    "            remove = True\n",
    "            break\n",
    "    if remove == False:\n",
    "        X_good.append(X[i])\n",
    "        \n",
    "X_good = np.array(X_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steffen/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# --- Replacing -8 with k-nearest neighbours (average) ---\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_good_std = scaler.fit_transform(X_good)\n",
    "\n",
    "X_no_8 = np.copy(X)\n",
    "\n",
    "kNN = 5\n",
    "\n",
    "cols_with_8 = [1,8,14,17,18,19,20,21,22]\n",
    "\n",
    "for q in cols_with_8:\n",
    "    for i in range(samples):\n",
    "\n",
    "        if X[i][q] == -8:\n",
    "            row_to_comp = []\n",
    "            mask = []\n",
    "            scaled = scaled_row(X[i])\n",
    "            for j in range(features):\n",
    "                if X[i][j] >= 0:\n",
    "                    mask.append(1)\n",
    "                    row_to_comp.append(scaled[j])\n",
    "                else:\n",
    "                    mask.append(0)\n",
    "            row_to_comp = np.array(row_to_comp)\n",
    "            mask = np.array(mask)\n",
    "            \n",
    "            # -- Getting the array of samples without special values in the good datasets-- \n",
    "            \n",
    "            S = masked_arr(X_good_std, mask)\n",
    "\n",
    "            imputed = predict_feature_mean(row_to_comp, S, kNN, X_good_std, q)\n",
    "\n",
    "            X_no_8[i][q] = imputed*scaler.scale_[q] + scaler.mean_[q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Creating one whole dataset ---\n",
    "data_set = np.append(y,np.copy(X_no_8),axis=1)\n",
    "#np.place(data_set, data_set == -7, 150)\n",
    "\n",
    "# --- Write intermediate result to file ---\n",
    "new_df = pd.DataFrame(data_set)\n",
    "new_df.to_csv(\"working_data_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelError(Exception):\n",
    "    pass\n",
    "\n",
    "def predict_values_lin_reg(X_tr,y_tr,X_test):\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred = model.predict(X_test)\n",
    "    return pred\n",
    "\n",
    "def data_spliter(all_data,target_col,target_val):\n",
    "    y = all_data[:,target_col:target_col+1]\n",
    "    X = np.delete(all_data,target_col,1)\n",
    "    \n",
    "    # Will hold the X for the y values that need to be predicted\n",
    "    X_target = np.zeros((1,X.shape[1]))\n",
    "    \n",
    "    row_no = 0 \n",
    "    for val in y:\n",
    "        if (val[0] == target_val):\n",
    "            X_target = np.append(X_target,X[row_no:row_no+1,:],axis=0)\n",
    "            X = np.delete(X, row_no, 0)\n",
    "            y = np.delete(y, row_no, 0) \n",
    "        else:\n",
    "            row_no += 1\n",
    "            \n",
    "    X_target = np.delete(X_target,0,0)\n",
    "    \n",
    "    return X,y,X_target\n",
    "\n",
    "def combine_parts_inorder(X,y,X_target,y_target,target_col):\n",
    "    y_target = y_target.reshape((y_target.shape[0],1))\n",
    "    y_full = np.append(y_target,y,axis=0)\n",
    "    X_full = np.append(X_target,X,axis=0)\n",
    "    \n",
    "    data = np.append(X_full[:,:target_col],y_full,axis=1)\n",
    "    data = np.append(data,X_full[:,target_col:],axis=1)\n",
    "    return data\n",
    "\n",
    "def average_each_feature(X):\n",
    "    X_target = np.zeros((1,X.shape[1]))\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        col = X[:,i]\n",
    "        col = np.mean(col,axis=0)\n",
    "        print(col)\n",
    "        X_target[:,i] = col\n",
    "        \n",
    "    return X_target\n",
    "\n",
    "\n",
    "def process_and_predict(all_data,target_col,target_val,exclude=None,model=\"linear\"):\n",
    "    \n",
    "    # -- Split data --\n",
    "    X,y,X_target = data_spliter(all_data,target_col,target_val)\n",
    "    \n",
    "    # -- Remove certain columns --\n",
    "    if (exclude != None):\n",
    "        y_tr = np.copy(y)\n",
    "        X_tr = remove_col_with_vals(X,exclude)\n",
    "        X_pred = remove_col_with_vals(X_target,exclude) # The x used to predict\n",
    "    else:\n",
    "        y_tr = np.copy(y)\n",
    "        X_tr = np.copy(X)\n",
    "        X_pred = np.copy(X_target)\n",
    "        \n",
    "    # -- Run regression --\n",
    "    if (model == \"linear\"):\n",
    "        y_target = predict_values_lin_reg(X_tr,y_tr,X_pred)\n",
    "    \n",
    "    elif (model == \"polynomial\"):\n",
    "        pass\n",
    "    \n",
    "    elif (model == \"special\"):\n",
    "        X_avg = average_each_feature(X_pred)\n",
    "        pred = predict_values_lin_reg(X_tr,y_tr,X_avg)\n",
    "        print(pred)\n",
    "    \n",
    "    else:\n",
    "        raise ModelError(\"Model currently not available\")\n",
    "        \n",
    "    final_data = combine_parts_inorder(X,y,X_target,y_target,target_col)\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Splitting again to training and target data (to avoid data leaking) --- \n",
    "\n",
    "y = data_set[:,:1]\n",
    "X = data_set[:,1:]\n",
    "\n",
    "# --- Finding the missing -9 values ---\n",
    "X = process_and_predict(X,0,-9,[-7])\n",
    "\n",
    "# --- Finding the missing -7 values ---\n",
    "X = process_and_predict(X,8,-7,[-7],None,\"special\")\n",
    "X = process_and_predict(X,14,-7,None,\"special\")\n",
    "\n",
    "# --- Rounding the values to nearest whole value ---\n",
    "X = np.around(X.astype(np.float),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Creating one whole dataset ---\n",
    "data_set = np.append(y,np.copy(X),axis=1)\n",
    "\n",
    "# --- Write final result to file ---\n",
    "new_df = pd.DataFrame(data_set)\n",
    "new_df.to_csv(\"data_weighted5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 10  8]\n",
      " [ 2 11  7]\n",
      " [ 5 12  6]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[2,11,7],[1,10,8],[5,12,6]])\n",
    "# a.sort(axis=0)\n",
    "a = a[a[:,0].argsort()]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
