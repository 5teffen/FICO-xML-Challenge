{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "import numpy.polynomial.polynomial as poly\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "import copy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"xML Challenge Dataset and Data Dictionary/heloc_dataset_v1.csv\")\n",
    "feature_names = list(df)\n",
    "data = df.values\n",
    "\n",
    "y_original= data[:,:1]\n",
    "X_original = data[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Basic Functions for Matrix editing --- \n",
    "\n",
    "def remove_row_with_all_same_val(data, target, val):\n",
    "    row_no = 0 \n",
    "    for row in data:\n",
    "        for col in row:\n",
    "            if (col == val):\n",
    "                remove = True\n",
    "            else:\n",
    "                remove = False\n",
    "                break\n",
    "\n",
    "        if remove:\n",
    "            data = np.delete(data, row_no, 0)\n",
    "            target = np.delete(target, row_no, 0)     \n",
    "\n",
    "        else:\n",
    "            row_no += 1\n",
    "    \n",
    "    return data,target\n",
    "\n",
    "def remove_row_with_vals(data, target, vals):\n",
    "    row_no = 0 \n",
    "    for row in data:\n",
    "        for col in row:\n",
    "            if (col in vals):\n",
    "                data = np.delete(data, row_no, 0)\n",
    "                target = np.delete(target, row_no, 0) \n",
    "                row_no -= 1\n",
    "                break\n",
    "        row_no += 1\n",
    "    return data,target\n",
    "\n",
    "\n",
    "def remove_col_with_vals(data, vals):\n",
    "    no_cols = data.shape[1]\n",
    "    no_rows = data.shape[0]\n",
    "    row = 0\n",
    "    while (no_rows > row):\n",
    "        col = 0\n",
    "        while (no_cols > col):\n",
    "            if (data[row][col] in vals):\n",
    "                data = np.delete(data, col, 1)\n",
    "                no_cols -= 1\n",
    "            else:\n",
    "                col += 1\n",
    "        row += 1     \n",
    "    return data\n",
    "\n",
    "\n",
    "def scaled_row(row):\n",
    "    scld = []\n",
    "    for k in range(features):\n",
    "        scld.append((row[k] - scaler.mean_[k])/scaler.scale_[k])\n",
    "    scld = np.array(scld)\n",
    "    return scld\n",
    "        \n",
    "        \n",
    "def masked_arr(A, mask):\n",
    "    B = []\n",
    "    for i in range(len(A)):\n",
    "        row = []\n",
    "        for j in range(len(A[0])):\n",
    "            if mask[j] != 0:\n",
    "                row.append(A[i][j])\n",
    "        B.append(row)\n",
    "    B = np.array(B)\n",
    "    return B\n",
    "\n",
    "def distance(row1, row2):\n",
    "    dist = 0\n",
    "    for i in range(len(row1)):\n",
    "        t = (row1[i]-row2[i])**2\n",
    "        dist += t\n",
    "    dist = np.sqrt(dist)\n",
    "    return dist\n",
    "\n",
    "# predict features using kNN imputation\n",
    "# need to test for both weighted and simple mean\n",
    "# using 3 or 5 neighbors\n",
    "def predict_feature_weighted(row, C, k, originalArr, ft_idx):\n",
    "    \n",
    "    distances = []\n",
    "    for i in range(len(C)):\n",
    "        distances.append(distance(row,C[i]))\n",
    "    distances = np.array(distances)\n",
    "    \n",
    "    max_dist = np.max(distances)\n",
    "        \n",
    "    idx = np.argpartition(distances, k)\n",
    "#     print(idx)\n",
    "    \n",
    "    values = []\n",
    "    min_dists = []\n",
    "    for i in range(k):\n",
    "        values.append(originalArr[idx[i]][ft_idx])\n",
    "#         print(Z[idx[i]])\n",
    "        min_dists.append(distances[idx[i]])\n",
    "    values = np.array(values) \n",
    "    min_dists = np.array(min_dists)\n",
    "    \n",
    "#     max_dist = np.max(min_dists)\n",
    "\n",
    "    weights = []\n",
    "    for i in min_dists:\n",
    "        weights.append(1 - (i/max_dist))\n",
    "    \n",
    "#     print(weights)\n",
    "#     print(values)\n",
    "        \n",
    "    imputed_val = 0\n",
    "    for i in range(len(weights)):\n",
    "        imputed_val += weights[i] * values[i]\n",
    "#         print(imputed_val)\n",
    "        \n",
    "    return imputed_val         \n",
    "\n",
    "def predict_feature_mean(row, C, k, originalArr,ft_idx):\n",
    "    \n",
    "    distances = []\n",
    "    for i in range(len(C)):\n",
    "        distances.append(distance(row,C[i]))\n",
    "    distances = np.array(distances)\n",
    "        \n",
    "    idx = np.argpartition(distances, k)\n",
    "#     print(idx)\n",
    "    \n",
    "    values = []\n",
    "    min_dists = []\n",
    "    for i in range(k):\n",
    "        values.append(originalArr[idx[i]][ft_idx])\n",
    "#         print(Z[idx[i]])\n",
    "        min_dists.append(distances[idx[i]])\n",
    "    values = np.array(values) \n",
    "    min_dists = np.array(min_dists)\n",
    "    \n",
    "#     print(values)\n",
    "        \n",
    "    imputed_val = 0\n",
    "    for i in range(len(values)):\n",
    "        imputed_val += values[i]/(len(values))\n",
    "#         print(imputed_val)\n",
    "        \n",
    "    return imputed_val          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_model_degrees(X_tr,y_tr,degrees):\n",
    "    RSS_list = []\n",
    "    for deg in degrees:\n",
    "        model = poly.polyfit(X_tr,y_tr,degree)\n",
    "        pred = poly.polyval(X_tr, model)\n",
    "        RSS = np.mean((Y_tr-pred)**2)\n",
    "        RSS_list.append(RSS)\n",
    "        \n",
    "        \n",
    "    plt.figure()\n",
    "    plt.plot(degrees,RSS_list,'.-',color='r') \n",
    "    plt.xlabel('Degree')                            \n",
    "    plt.ylabel('Performance')\n",
    "    \n",
    "\n",
    "def predict_values_poly_reg(X_tr,y_tr,X_test,deg):\n",
    "    model = poly.polyfit(X_tr,y_tr,degree)\n",
    "    pred = poly.polyval(X_test, model)\n",
    "    return pred\n",
    "\n",
    "def predict_values_lin_reg(X_tr,y_tr,X_test):\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred = model.predict(X_test)\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10459, 23)\n",
      "(9871, 23)\n",
      "(10459, 1)\n",
      "(9871, 1)\n"
     ]
    }
   ],
   "source": [
    "# --- Removing the rows with only -9 values ---\n",
    "X, y = remove_row_with_all_same_val(X_original,y_original,-9)\n",
    "\n",
    "# --- Testing ---\n",
    "print(X_original.shape)\n",
    "print(X.shape)\n",
    "print(y_original.shape)\n",
    "print(y.shape)\n",
    "\n",
    "Y = np.copy(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Taking the array without any negatives present ---\n",
    "\n",
    "samples, features = Y.shape\n",
    "\n",
    "Z = []\n",
    "\n",
    "for i in range(samples):\n",
    "    remove = False\n",
    "    for j in range(features):\n",
    "        if Y[i][j] < 0:\n",
    "            remove = True\n",
    "            break\n",
    "    if remove == False:\n",
    "        Z.append(Y[i])\n",
    "        \n",
    "Z = np.array(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steffen/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# --- Replacing -8 with k-nearest neighbours (average) ---\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Z_std = scaler.fit_transform(Z)\n",
    "\n",
    "imputed_8_Y = np.copy(Y)\n",
    "# imputed_8_Y_std = np.copy(Y_std)\n",
    "\n",
    "k = 5\n",
    "\n",
    "cols_with_8 = [1,8,14,17,18,19,20,21,22]\n",
    "\n",
    "for q in cols_with_8:\n",
    "    for i in range(samples):\n",
    "\n",
    "        if Y[i][q] == -8:\n",
    "\n",
    "#             print(Y[i])\n",
    "\n",
    "            row_to_comp = []\n",
    "            mask = []\n",
    "            scaled = scaled_row(Y[i])\n",
    "            for j in range(features):\n",
    "                if Y[i][j] >= 0:\n",
    "                    mask.append(1)\n",
    "                    row_to_comp.append(scaled[j])\n",
    "                else:\n",
    "                    mask.append(0)\n",
    "            row_to_comp = np.array(row_to_comp)\n",
    "            mask = np.array(mask)\n",
    "\n",
    "            S = masked_arr(Z_std, mask)\n",
    "\n",
    "    #         print(row_to_comp.shape)\n",
    "    #         print(S.shape)\n",
    "\n",
    "    #         print(row_to_comp)\n",
    "    #         print(S)\n",
    "\n",
    "            imputed = predict_feature_mean(row_to_comp, S, k, Z_std, q)\n",
    "#             print(imputed)\n",
    "#             print(imputed*scaler.scale_[q] + scaler.mean_[q])\n",
    "\n",
    "            imputed_8_Y[i][q] = imputed*scaler.scale_[q] + scaler.mean_[q]\n",
    "\n",
    "    #         imputed_8_Z_std[i][1] = imputed\n",
    "\n",
    "#             print(imputed_8_Y[i])\n",
    "    #         print(imputed_8_Z_std[i])\n",
    "    \n",
    "#             print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_no8 = np.append(y,np.copy(imputed_8_Y),axis=1)\n",
    "np.place(data_no8, data_no8 == \"Bad\", 0)\n",
    "np.place(data_no8, data_no8 == \"Good\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_organizer(all_data,target_col,target_val,no_col=0):\n",
    "    y = all_data[:,target_col:target_col+1]\n",
    "    X = np.delete(all_data,target_col,1)\n",
    "    \n",
    "    # Will hold the X for the y values that need to be predicted\n",
    "    X_target = np.zeros((1,X.shape[1]))\n",
    "    \n",
    "    row_no = 0 \n",
    "    for val in y:\n",
    "        if (val[0] == target_val):\n",
    "            X_target = np.append(X_target,X[row_no:row_no+1,:],axis=0)\n",
    "            X = np.delete(X, row_no, 0)\n",
    "            y = np.delete(y, row_no, 0) \n",
    "        else:\n",
    "            row_no += 1\n",
    "            \n",
    "    X_target = np.delete(X_target,0,0)\n",
    "    \n",
    "    # Can make the function even more comprehensive\n",
    "    if (no_col == 0):\n",
    "        pass\n",
    "    \n",
    "    return X,y,X_target\n",
    "\n",
    "def combine_parts(X,y,X_target,y_target):\n",
    "    half_1 = y_target.reshape((y_target.shape[0],1))\n",
    "    half_1 = np.append(half_1,X_target,axis=1)\n",
    "    half_2 = np.append(y,X,axis=1)\n",
    "    \n",
    "    data = np.append(half_1,half_2,axis = 0)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_nine,y_nine,X_nine_missing = data_organizer(data_no8,1,-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Creating a training and test case from the column with outstanding -9 values---\n",
    "\n",
    "# \"\"\"\n",
    "# y_nine\n",
    "# X_nine\n",
    "# X_nine_missing\n",
    "# \"\"\"\n",
    "# y_nine = data_no8[:,1:2]\n",
    "# X_nine = np.delete(data_no8,1,1)\n",
    "\n",
    "# nine = -9\n",
    "\n",
    "# X_nine_missing = np.zeros((1,X_nine.shape[1]))\n",
    "\n",
    "# row_no = 1 \n",
    "# for val in y_nine:\n",
    "#     if (val[0] == nine):\n",
    "#         X_nine_missing = np.append(X_nine_missing,X_nine[row_no:row_no+1,:],axis=0)\n",
    "\n",
    "#         X_nine = np.delete(X_nine, row_no, 0)\n",
    "#         y_nine = np.delete(y_nine, row_no, 0) \n",
    "#     else:\n",
    "#         row_no += 1\n",
    "# X_nine_missing = np.delete(X_nine_missing,0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Removing the columns with -7 values---\n",
    "y_nine_tr = np.copy(y_nine)\n",
    "X_nine_tr = remove_col_with_vals(X_nine,[-7])\n",
    "X_nine_test = remove_col_with_vals(X_nine_missing,[-7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 21)\n",
      "(9861, 21)\n",
      "(9861, 1)\n"
     ]
    }
   ],
   "source": [
    "# --- Testing ---\n",
    "print(X_nine_test.shape)\n",
    "print(X_nine_tr.shape)\n",
    "print(y_nine_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9871, 24)\n"
     ]
    }
   ],
   "source": [
    "# --- Running linear regression to find missing -9 ---\n",
    "y_nine_missing = predict_values_lin_reg(X_nine_tr,y_nine_tr,X_nine_test)\n",
    "\n",
    "# --- Piecing it together ---\n",
    "\n",
    "data_no89 = combine_parts(X_nine,y_nine,X_nine_missing,y_nine_missing)\n",
    "# results = results.reshape((results.shape[0],1)) \n",
    "# results = np.append(results,X_nine_missing,axis=1)\n",
    "# other = np.append(y_nine,X_nine,axis=1)\n",
    "\n",
    "# X_no89 = np.append(results,other,axis = 0)\n",
    "\n",
    "print(data_no89.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4664, 22)\n",
      "(5207, 22)\n",
      "(5207, 1)\n"
     ]
    }
   ],
   "source": [
    "# --- Selecting the first column with -7  ---\n",
    "\n",
    "X_seven,y_seven,X_seven_missing = data_organizer(data_no89,9,-7)\n",
    "\n",
    "# --- Removing the other column with -7 values---\n",
    "y_seven_tr = np.copy(y_seven)\n",
    "X_seven_tr = remove_col_with_vals(X_seven,[-7])\n",
    "X_seven_test = remove_col_with_vals(X_seven_missing,[-7])\n",
    "\n",
    "print(X_seven_test.shape)\n",
    "print(X_seven_tr.shape)\n",
    "print(y_seven_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model_degrees(X_seven_tr,y_seven_tr,[degrees]):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Running linear regression to find missing -9 --\n",
    "y_seven_missing = predict_values_lin_reg(X_seven_tr,y_seven_tr,X_seven_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9871, 24)\n"
     ]
    }
   ],
   "source": [
    "data_no89 = combine_parts(X_seven,y_seven,X_seven_missing,y_seven_missing)\n",
    "print(data_no89.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1855, 23)\n",
      "(8016, 23)\n",
      "(8016, 1)\n"
     ]
    }
   ],
   "source": [
    "# --- Selecting the second column with -7  ---\n",
    "\n",
    "X_seven,y_seven,X_seven_missing = data_organizer(data_no89,15,-7)\n",
    "\n",
    "# --- Removing the other column with -7 values---\n",
    "y_seven_tr = np.copy(y_seven)\n",
    "X_seven_tr = np.copy(X_seven)\n",
    "X_seven_test = np.copy(X_seven_missing)\n",
    "\n",
    "print(X_seven_test.shape)\n",
    "print(X_seven_tr.shape)\n",
    "print(y_seven_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Running linear regression to find missing -9 --\n",
    "y_seven_missing = predict_values_lin_reg(X_seven_tr,y_seven_tr,X_seven_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.748628528655166 56.39038425695428 75.95160492833523 ..., 1.8\n",
      "  2.8000000000000003 81.0]\n",
      " [8.249145749738885 60.232763974821054 85.39024838260647 ..., 1.8\n",
      "  2.8000000000000003 82.200000000000003]\n",
      " [5.022676224208863 46.55772837014838 88 ..., 3 0 71]\n",
      " ..., \n",
      " [7 80 73 ..., 2 0 100]\n",
      " [1 28 65 ..., 2 1 80]\n",
      " [6 35 72 ..., 1 0 38]]\n"
     ]
    }
   ],
   "source": [
    "X_processed = combine_parts(X_seven,y_seven,X_seven_missing,y_seven_missing)\n",
    "print(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.around(X_processed, decimals=2)\n",
    "X_processed = np.around(X_processed.astype(np.float),0)\n",
    "#np.ndarray.round(X_processed,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.savetxt(\"test_1.csv\", X_processed, delimiter=\",\")\n",
    "\n",
    "new_df = pd.DataFrame(X_processed)\n",
    "\n",
    "new_df.to_csv(\"test_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
